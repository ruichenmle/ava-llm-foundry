integrations:
- integration_type: git_repo
  git_repo: ruichenmle/ava-llm-foundry
  git_branch: llama2-pretrain # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[gpu]
  ssh_clone: false # Should be true if using a private repo

# We are fetching, converting, and training on the 'val' split
# as it is small and quick to get going for this demo.
# For real training runs, follow the instructions in `llm-foundry/scripts/train/README.md`
# to convert and host the full 'train' dataset.

command: |
  cd ava-llm-foundry/scripts/
  huggingface-cli login --token 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
  pip install awscli
  aws s3 cp s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/productGPT/data/39_61_62/train/ my-local-data-train/ --recursive
  python data_prep/convert_dataset_json.py --path my-local-data-train/ \
    --out_root s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/productGPT/data/39_61_62/llama2-13b-4096-streaming/train/ \
    --tokenizer meta-llama/Llama-2-13b-hf \
    --concat_tokens 4096 \
    --eos_text '</s>'

  aws s3 cp s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/productGPT/data/39_61_62/val/ my-local-data-val/ --recursive
  python data_prep/convert_dataset_json.py --path my-local-data-val/ \
    --out_root s3://mosaicml-68c98fa5-0b21-4c7b-b40b-c4482db8832a/productGPT/data/39_61_62/llama2-13b-4096-streaming/val/ \
    --tokenizer meta-llama/Llama-2-13b-hf \
    --concat_tokens 4096 \
    --eos_text '</s>'  


image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
# Mosaic Cloud will use run_name (with a unique suffix) to populate the env var $COMPOSER_RUN_NAME


gpu_num: 8
gpu_type: a100_80gb
cluster: r8z6 # replace with your cluster here!


# Run Name
run_name: productGPT-data-llama2-4096-conversion # If left blank, will be read from env var $RUN_NAME


parameters: {}